# ============================================================================
# Pricey Monorepo Environment Configuration
# ============================================================================
# This is the master environment configuration file for the entire monorepo.
# Copy this file to .env.local and customize for your local development.
#
# All services (api-gateway, ocr-service, web) will load variables from this
# root .env.local file to avoid duplication and ensure consistency.
# ============================================================================

# ============================================================================
# SHARED CONFIGURATION (Used by multiple services)
# ============================================================================

# Database Configuration (Used by: api-gateway, ocr-service, database package)
DATABASE_URL="postgresql://pricey:pricey_dev_password@localhost:5432/pricey"

# Redis Configuration (Used by: api-gateway, ocr-service)
REDIS_URL="redis://localhost:6379"

# MinIO/S3 Storage Configuration (Used by: api-gateway, ocr-service)
S3_ENDPOINT="localhost"
S3_PORT="9000"
S3_ACCESS_KEY="minioadmin"
S3_SECRET_KEY="minioadmin"
S3_USE_SSL="false"
S3_BUCKET="pricey-receipts"

# Application Environment (Used by: all services)
NODE_ENV="development"
LOG_LEVEL="info"

# ============================================================================
# API GATEWAY CONFIGURATION (apps/api-gateway)
# ============================================================================

# Server Configuration
PORT="3001"
HOST="0.0.0.0"

# CORS Configuration
CORS_ORIGIN="*"

# Rate Limiting
RATE_LIMIT_MAX="100"
RATE_LIMIT_WINDOW="60000"

# ============================================================================
# OCR SERVICE CONFIGURATION (apps/ocr-service)
# ============================================================================

# OCR Processing Configuration
OCR_CONCURRENCY="5"
OCR_TIMEOUT="30000"

# LLM Parser Configuration
LLM_PROVIDER="ollama"  # Options: ollama, github, openai

# LLM Base URL Options (for Ollama):
# - http://localhost:11434 (Docker Ollama - slower, no GPU acceleration)
# - http://localhost:10000 (Mac's local Ollama with GPU - 10-20x faster!)
#   For Mac: brew install ollama && ollama serve --host 0.0.0.0:10000
#   Or use Msty (https://msty.app) which runs Ollama automatically
LLM_BASE_URL="http://localhost:11434"

# Vision model for receipt parsing (llava, llama3.2-vision:11b, moondream)
LLM_MODEL="llava"
LLM_TIMEOUT="60000"
LLM_TEMPERATURE="0.1"

# Docker auto-download model (for docker-compose.yml)
OLLAMA_MODEL="llava"

# GitHub Models Configuration (for GitHub Copilot users)
# Get your token from: https://github.com/settings/tokens
# Required permissions: Use GitHub Copilot (automatically included with Copilot subscription)
# GITHUB_TOKEN="ghp_your_token_here"
# GITHUB_MODEL="gpt-4o"  # Options: gpt-4o, gpt-4o-mini

# ============================================================================
# WEB APP CONFIGURATION (apps/web)
# ============================================================================

# API Gateway URL (Public - exposed to browser)
NEXT_PUBLIC_API_URL="http://localhost:3001"

# ============================================================================
# PRODUCTION DOCKER CONFIGURATION (docker-compose.prod.yml)
# ============================================================================
# These variables are only used for production Docker deployments.
# Change all passwords before deploying to production!

# PostgreSQL Production
POSTGRES_USER="pricey"
POSTGRES_PASSWORD="CHANGE_ME_IN_PRODUCTION"
POSTGRES_DB="pricey"
POSTGRES_PORT="5432"

# Redis Production
REDIS_PASSWORD="CHANGE_ME_IN_PRODUCTION"
REDIS_PORT="6379"

# MinIO Production
MINIO_ROOT_USER="CHANGE_ME_IN_PRODUCTION"
MINIO_ROOT_PASSWORD="CHANGE_ME_IN_PRODUCTION"

# API Production
API_PORT="3001"
