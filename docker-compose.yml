# Docker Compose for Pricey Development
# Copyright (C) 2025 Matthias Wallner-GÃ©hri
# Licensed under AGPL-3.0

version: '3.9'

services:
  # PostgreSQL Database
  postgres:
    image: postgres:18-alpine
    container_name: pricey-postgres
    environment:
      POSTGRES_USER: pricey
      POSTGRES_PASSWORD: pricey_dev_password
      POSTGRES_DB: pricey
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - '5432:5432'
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U pricey']
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - pricey-network

  # Redis Cache
  redis:
    image: redis:8-alpine
    container_name: pricey-redis
    ports:
      - '6379:6379'
    healthcheck:
      test: ['CMD', 'redis-cli', 'ping']
      interval: 10s
      timeout: 3s
      retries: 5
    networks:
      - pricey-network

  minio:
    image: minio/minio:latest
    container_name: pricey-minio
    command: server /data --console-address ':9001'
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - '9000:9000'
      - '9001:9001'
    volumes:
      - minio_data:/data
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:9000/minio/health/live']
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - pricey-network

  # Ollama - Self-hosted LLM for receipt parsing (OPTIONAL)
  # Enable with: docker-compose --profile ollama up -d
  # Or set COMPOSE_PROFILES=ollama in .env.local
  ollama:
    image: ollama/ollama:latest
    container_name: pricey-ollama
    profiles:
      - ollama
    ports:
      - '11434:11434'
    volumes:
      - ollama_data:/root/.ollama
      - ./docker/ollama-entrypoint.sh:/entrypoint.sh:ro
    environment:
      # Allow access from other containers
      OLLAMA_HOST: 0.0.0.0:11434
      # Vision model to auto-download on startup (set to empty to skip)
      # Vision options: llava, llama3.2-vision:11b, llama3.2-vision:90b, moondream
      # Text-only options: llama3.2:1b, llama3.2:3b, mistral:7b, phi3:mini
      # Set OLLAMA_MODEL="" to disable auto-download
      OLLAMA_MODEL: ${OLLAMA_MODEL:-}
    entrypoint: ['/bin/bash', '/entrypoint.sh']
    healthcheck:
      test: ['CMD', 'ollama', 'list']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    networks:
      - pricey-network
    deploy:
      resources:
        # Optional: Uncomment if you have NVIDIA GPU
        # reservations:
        #   devices:
        #     - driver: nvidia
        #       count: 1
        #       capabilities: [gpu]
        limits:
          # Limit memory usage (adjust based on your system)
          memory: 8G

volumes:
  postgres_data:
    driver: local
  minio_data:
    driver: local
  ollama_data:
    driver: local

networks:
  pricey-network:
    driver: bridge
